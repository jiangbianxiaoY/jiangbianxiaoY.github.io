[{"title":"ResNet学习","url":"/2025/10/15/ResNet%E5%AD%A6%E4%B9%A0/","content":"ResNet残差神经网络一、背景​\t\t对于一个神经网络模型，越多的层数越多的参数往往可以具有很好的效果，但是当神经网络层数变多后，在反向传播的时候就会出现梯度消失或者梯度爆炸的现象。而残差网络的主要贡献就是现了“退化现象（Degradation）”，并针对退化现象发明了 “直连边&#x2F;短连接（Shortcut connection）”，极大的消除了深度过大的神经网络训练困难问题。神经网络的“深度”首次突破了100层、最大的神经网络甚至超过了1000层。\n​\t\t残差神经网络将深度学习网络的层数推进到了100层而当时的VGG网络模型的层数为19层。\n一、什么是退化现象​\t\t随着CNN的发展和普及，人们发现增加神经网络的层数可以提高训练精度，但是如果只是单纯的增加网络的深度，可能会出现“梯度弥散”和“梯度爆炸”等问题。传统的解决方法则是权重的初始化(normalized initializatiton)和批标准化(batch normlization)，虽然解决了梯度问题，但是深度加深了，却带来了另外的问题，就是网络性能的退化现象，可以简单的理解为，随着训练轮数（epoch）的增加，精度到达一定程度后，就开始下降了。\n二、为什么深度过大而神经网络难以训练？​\t\t原因也很简单主要就是出现了梯度消失和梯度爆炸的现象，其他原因比如说是计算资源的消耗，这类原因可以通过GPU集群来解决，而过拟合的问题，可以通过采集海量的数据集和Dropout正则化可以有效解决，梯度消失和爆炸虽然也可以利用Batch Normalization解决，但根据实验发现并不如人愿望。\n​\t\tBatch Normalization（批量归一化）：由Google在2015年提出，是近年来DL（深度学习）领域最重要的进步之一，该方法依靠两次连续的线性变换，希望转化后的数值满足一定的特性（分布），不仅可以加快了模型的收敛速度，也一定程度缓解了特征分布较散的问题，使深度神经网络（DNN）训练更快、更稳定。（这里讲的相对松散一些，详细的内容以及原理大家可以参考其他博客），具体的过程就是通过方法将该层的特征值分布重新拉回到标准正态分布，特征值降落在激活函数对于输入较为敏感的区间，输入的小变化可导致损失函数较大的变化，使得梯度变大，避免梯度消失，同时也可加快收敛。\n​\t\t**梯度消失：**我们知道神经网络在进行反向传播(BP算法)的时候会对参数W进行更新，梯度消失就是靠后面网络层(如layer3)能够正常的得到一个合理的偏导数，但是靠近输入层的网络层，计算的到的偏导数近乎零，W几乎无法得到更新。\n原因：反向传播的时候的链式法则，越是浅层的网络，其梯度表达式可以展现出来连乘的形式，而这样如果都是小于1的，这样的话，浅层网络参数值的更新就会变得很慢，这就导致了深层网络的学习就等价于了只有后几层的浅层网络的学习了。\n​\t\t**梯度爆炸：**靠近输入层的网络层，计算的到的偏导数极其大，更新后W变成一个很大的数(爆炸)。\n原因：类似于上述的原因，假如都是大于1的时候，那么浅层的网络的梯度过大，更新的参数变量也过大，所以无论是梯度消失还是爆炸都是训练过程会十分曲折的，都应该尽可能避免。\n三、残差网络解决的问题​\t\t神经网络越来越深的时候，反传回来的梯度之间的相关性会越来越差，最后接近白噪声。因为我们知道图像具有局部相关性，可以认为梯度也应该具备类似的相关性，这样更新的梯度才有意义，如果梯度接近白噪声，那梯度更新可能根本就是在做随机扰动。基于这种退化问题，作者通过浅层网络等同映射构造深层模型，结果深层模型并没有比浅层网络有等同或更低的错误率，推断退化问题可能是因为深层的网络并不是那么好训练，也就是求解器很难去利用多层网络拟合同等函数。\n​\t\t如果深层网络的后面那些层是恒等映射，那么模型就退化为一个浅层网络。那当前要解决的就是学习恒等映射函数了。这段话也是可以这样理解一下，如果更深层次的网络都能够学习到恒等映射，那么准确率起码不会降低，所以这正是残差网络的解决的问题。\n二、残差块原理​\t\t一个残差快的数学模型如下图所示，残差网络和之前的网络最大的不同之处就是多了一条自身的捷径分支，正是因为这一个分支的存在，使得网络在反向传播的时候，损失可以通过这条捷径将梯度直接传向更前的网络，从而减缓网络退化问题，我们前边了解到梯度之间具有相关性的。我们在有个梯度相关性这个指标之后，作者分析了许多的结构和激活函数，发现了正是我们这个网络在保持梯度相关性上是很强的，除了这一点之外，残差网络并没有增加新的参数，只是多了一步加法，计算量相对而言也没有增加。\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n​\t\t数据流从残差块的上方进行输入X,进行到卷积核进行正向转播得到了F(x)，这个和普通的卷积神经网络是一致的，但是残差连接会将在残差块输出的时候将输入的X数据加入到输出当中。$H(x) &#x3D; F(x) + x$ ，H(x)就是这个残差块的输出。\n​\t\t在反向传播的时候，除了会对于卷积部分进行梯度求导，也会绕过这个残差块将逆向数据传递给到上一个残差块当中作为输入，从而稳定梯度求导部分降低梯度消失，这样增加层数。\n三、残差块数据流设定讨论​\t\t可能这有一个问题，为什么在上文的数据跳跃输出的时候是残差块的输入。为啥不是X&#x2F;2或者X&#x2F;3更甚着2*X的输出作为下一层的输入呢？\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n​\t\t这个在ResNet卷积神经网络作者的另一篇文章中提到，还是原始的X作为跳跃连接的输入输出数据效果最好。\n四、残差网络​\t\t残差网络是由一系列的残差块加上一些其他层组成的，残差快由两部分组成直接映射部分和残差部分。\n​\t\t\t\t\t\t\t\t\t\t\n​\t需要注意的是残差块内含有卷积核，卷积核在不同层块的通道数量是不同的，可能会上升通道数或者降低通道数导致F(X)与X的通道数是不同的，这里需要1*1的卷积核进行通道数规范，才可以将输出F(X)与X的数据格式一致。其他的与普通的CNN卷积神经网络一致，在最后一层的激活函数，再通过单一卷积核进行展开然后在激活进行输出。\n五、残差网络下的反向传播公式推导​\t\t我不会。\n","categories":["深度学习","计算机视觉"],"tags":["ResNet","卷积神经网络","图像识别","残差网络","深度学习"]}]